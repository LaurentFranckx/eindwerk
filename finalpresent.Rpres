Next Word Prediction 
========================================================

<style type="text/css">
.small-code pre code {
font-size: 1.3em;
}
</style>


Central objectives and constraints:


- Generate "next" word prediction for English sentence with 3 datasets: Twitter, news articles and blogs
- Differences between 3 datasets requires separate analysis 
- Underlying prediction algorithm should be accurate 
- Keep data size for on-line implementation within Shiny limits
- Quick response to user input: prediction within 1 sec
- Correct for user spelling mistakes with approximate matching 
- Default:  single word prediction but user can ask  3-word prediction


Data: sources and approach 
========================================================


- Stopwords are integral part of content: keep them 
- No stemming: users want complete word, not its stem
- Eliminate words and low order n-grams with low count (less than 4 occurences):   improves speed and reduces memory use  without decreasing accuracy 
- Algorithm optimised for 4-grams, but also uses prediction based on 3, 2- and 1-grams: backoff  with 'discounting' (see next slide)
- Link to app and user guidance:

https://lfranckx.shinyapps.io/CapVersion2/ 



</small>



Description of algorithm
========================================================

![alt text](algor_flow.PNG)





Average accuracy in validation sets
========================================================

```{r, echo=FALSE, fig.width=6,fig.height=3,dpi=300,out.width="1920px",height="1080px"}
library(ggplot2)
library(plyr)
library(reshape)
#load("D:/coursera/dsc_capstone/mean_accuracy.RData")
name_accumat <- "accuracy_test"
load(file = paste(name_accumat, "RData", sep="."))

accu_mat_twitter <- accu_mat[accu_mat$TrainCorpus== "twitter"&  accu_mat$i ==5, ]
accu_mat_news <- accu_mat[accu_mat$TrainCorpus== "news"&  accu_mat$i ==5, ]
accu_mat_blogs <- accu_mat[accu_mat$TrainCorpus== "blogs"&  accu_mat$i ==5, ]

library(ggplot2)

plot_train_test <- function(train_df, test){
  id_vars <- c("k","TestCorpus")
  meas_vars <- c("accu1","accu2","accu3")
  accu_mat <- train_df[train_df$TestCorpus == test,  ]
  accu_mat_twitter_plot <- accu_mat[ , c(id_vars,"accu1")]
  accu_mat_twitter_plot$words <- c(1)
  names(accu_mat_twitter_plot) <- gsub("accu1","Accuracy",names(accu_mat_twitter_plot))
  accu_mat_twitter_plot2 <- accu_mat[ , c(id_vars, "accu2")]
  accu_mat_twitter_plot2$words <- c(2)
  names(accu_mat_twitter_plot2) <- gsub("accu2","Accuracy",names(accu_mat_twitter_plot2))
  accu_mat_twitter_plot3 <- accu_mat[ , c(id_vars,"accu3")]
  accu_mat_twitter_plot3$words <- c(3)
  names(accu_mat_twitter_plot3) <- gsub("accu3","Accuracy",names(accu_mat_twitter_plot3))
  
  accu_mat_twitter_plot_new <- rbind(accu_mat_twitter_plot, accu_mat_twitter_plot2)
  accu_mat_twitter_plot_new <- rbind(accu_mat_twitter_plot_new, accu_mat_twitter_plot3)
  names(accu_mat_twitter_plot_new) <- gsub("k","Discount_factor",names(accu_mat_twitter_plot_new))
  
  accu_mat_twitter_plot_new$words <- as.factor(accu_mat_twitter_plot_new$words)
  names(accu_mat_twitter_plot_new) <- gsub("words","suggested_words",names(accu_mat_twitter_plot_new))
  return(accu_mat_twitter_plot_new)
  #ggplot(accu_mat_twitter_plot_new, aes(Discount_factor, Accuracy, colour = suggested_words)) + geom_point() + facet_grid(.  ~ TestCorpus)
  #ggplot(accu_mat_twitter_plot_new, aes(Discount_factor, Accuracy, colour = suggested_words)) + geom_point() 
  
}

twitplot <- plot_train_test(accu_mat_twitter, "blogs")
twitplot$TrainCorpus <- "twitter"
newsplot <- plot_train_test(accu_mat_news, "blogs")
newsplot$TrainCorpus <- "news"
blogplot <- plot_train_test(accu_mat_blogs, "news")
blogplot$TrainCorpus <- "blogs"

toplot <- rbind(twitplot,blogplot)
toplot <- rbind(toplot,newsplot)
ggplot(toplot, aes(Discount_factor, Accuracy, colour = suggested_words)) + geom_point() + facet_grid(.  ~ TrainCorpus)



```

<small>

Interpretation: see next slide

</small>

Impact of discount factor on accuracy
========================================================

- Validation sets: for each dataset, random sample of 500 n grams from other dataset ("blogs" for "news" and "twitter", "news" for "blogs")
- Validation accuracy of "blogs" independent of discount factor
- for "news": discount factor of 0.4: likelihood of words predicted by (n-1) gram has 60% of the weight of likelihood of words predicted by n-gram 
- for "twitter": discount factor of 0.2: likelihood of words predicted by (n-1) gram has 80% of the weight of likelihood of words predicted by n-gram 
- thus, in Twitter, more weight is attached to "lower" n-grams than in "news" database 
