---
title: "Milestone report"
author: "Me"
date: "Thursday, March 19, 2015"
output: html_document
---

#Introduction

This paper presents intermediate results in the construction of a predictive text model. The final objective
of this model is to provide users for suggestions for autocompleting a sentence they have started. 

The training data set has been obtained from the Coursera website, but are originally  from a corpus called HC Corpora (www.corpora.heliohost.org). The complete set consists of databases in English, German, Russian and Finnish. In this project we will limit ourselves to the English database. 

The English database itself is composed of three files, each composed of several hundreds of thousands of text lines from blogs, news cites and Twitter. 

As a first step in the paper, we will  explain the approach used to the cleaning of the data. Next, we will summarize the main descriptive statistics for each dataset, and explore how these compare to each other. Finally, we will describe the next steps we intend to undertake in order to construct our model. 


#Data cleaning

As a first step in our project, we have cleaned the original data. 

- First, we have modified the encoding from "UTF-8" to "latin1" - not doing so would lead to the creation of unreadable characters.
- Second, we have removed any numbers present.
- Third we have removed all the words that are banned by Google for their profane or offensive character.
- Fourth, we have removed all white spaces that have resulted frm the previous steps. 

Howevern, punctuation and other non-alphanumeric characters are an integral part of the sentence (especially in the case of Twitter), as have therefore been kept as they were.  

Although foreign words have crept inside the database, we have left them as they were. Indeed, some foreign words are deliberately used in English to convey a specific message. If foreign sentences have been included inadvertently in the database, they are likely to be eliminated in the next step of the analysis, where we have eliminated all words that did not occur a minimum number of times. 



#Descriptive statistics of the twitter file


```{r, echo=FALSE, cache=TRUE}
library("poweRlaw")

CutoffTable <- function(df, freq_val, value){
  df$sum_of_freq <- cumsum(df[, freq_val])
  total_count <- sum(df[, freq_val])
  df$sum_of_shares <- df$sum_of_freq/total_count
  df_low <- df[df$sum_of_shares < value,   ]
  return(list(df, total_count, df_low))
}
setwd("D:/coursera/dsc_capstone")
folder_txt <- file.path("Coursera-SwiftKey","final","en_US")
or_table <- readLines(file.path(folder_txt, "en_US.twitter.txt"))
TestFreqTable <- read.table("US.twitterunique.txt", stringsAsFactors = FALSE)
names(TestFreqTable) <- c("freq", "word")
TestFreqTable <- TestFreqTable[order(TestFreqTable$freq, decreasing = TRUE), ]
TestFreqList <- CutoffTable(TestFreqTable, "freq", 0.9)

min_fretable <- rbind(c(2, nrow(TestFreqTable[TestFreqTable$freq > 1, ])), c(3,nrow( TestFreqTable[TestFreqTable$freq > 2, ])))
min_fretable <- rbind(min_fretable, c(10, nrow(TestFreqTable[TestFreqTable$freq > 9, ])))
min_fretable <- rbind(min_fretable, c(100, nrow(TestFreqTable[TestFreqTable$freq > 99, ])))
min_fretable <- rbind(min_fretable, c(1000, nrow(TestFreqTable[TestFreqTable$freq > 999,])))
min_fretable <- rbind(min_fretable, c(10000, nrow(TestFreqTable[TestFreqTable$freq > 9999, ])))
min_fretable <- as.data.frame(min_fretable)
min_fretable$share <- min_fretable[, 2]/nrow(TestFreqTable) * 100
names(min_fretable) <- c("Minimum # of occurences", "# of unique words", "Share in total # of unique words")


```

The "uncleaned" version of the Twitter table contains `r sprintf("%1.1f", length(or_table)/10^6)` millions lines of text. 

When we limit the table to alphanumeric characters, it contains a total of
`r sprintf("%1.0f", TestFreqList[[2]]/10^6) ` million occurences of words, with  `r sprintf("%1.0f", nrow(TestFreqTable)/1000) ` thousand unique words.

Out of these unique words, `r sprintf("%0d", nrow(TestFreqList[[3]])) ` words (this is thus `r sprintf("%1.2f", nrow(TestFreqList[[3]])/nrow(TestFreqTable) * 100) ` %) correspond to 90% of all word occurences.

Another indication of the relative frequency with which words occur is to compare how many words 
appear a minimum number of times in the whole database: 

```{r, echo=FALSE}
#summary(TestFreqTable$freq)
rm(or_table)
gc()
min_fretable

```

Thus, `r nrow(TestFreqTable[TestFreqTable$freq > 1, ])` individual words appear more than once in the database,
`r nrow(TestFreqTable[TestFreqTable$freq > 2, ])` individual words appear more than twice in the database, etc.

The general skewness of the frequency of words is confirmed when we look at the summary statistics of the number of counts: 

```{r, echo=FALSE}
summary(TestFreqTable$freq)
```


Still another way to look at the data is to calculate how many individual words appear in the database with a given count. We represent here the first five and the last five lines of the table containing this information (henceforth the "frequency count table"). 



```{r, echo=FALSE, cache=TRUE}
freq_count <- TestFreqTable$freq
FreqPow <- displ$new(freq_count)
#FreqPow$getXmin()
#USBlogFreqPowPars <- estimate_pars(USBlogFreqPow, pars = NULL)
FreqPowMin <- estimate_xmin(FreqPow, pars = NULL)
FreqPow$setXmin(FreqPowMin)
FrqvsVal <- cbind(FreqPow$internal$freq,FreqPow$internal$values)
names(FrqvsVal ) <- c("Number of individuals words", "Number of counts in the whole database")
```

```{r, echo=FALSE, cache=TRUE}
head(FrqvsVal )
tail(FrqvsVal )
most_freq_word <- TestFreqTable[ TestFreqTable$freq == max(TestFreqTable$freq) , "word" ]
most_freq_word 
```

Thus `r sprintf("%1.0f",FrqvsVal [1, 1]/1000)` thousand individual words appear exactly `r FrqvsVal [1, 2]` time in the database, while `r FrqvsVal [nrow(FrqvsVal), 1]` word (not surprisingly, this is " `r most_freq_word` ""), appears exactly `r FrqvsVal [nrow(FrqvsVal), 2]` times in the database.

Finally, we can have a look at a sample of words that appear very infrequently in the database: 

```{r, echo=FALSE, cache=TRUE}
set.seed(123)
low_frq <- TestFreqTable[ TestFreqTable$freq < 7 , ]
low_frq[sample(1:nrow(low_frq), 10)  , ]
# head(TestFreqTable[ TestFreqTable$freq == min(TestFreqTable$freq) , ], 3)
# 
# head(TestFreqTable[ TestFreqTable$freq == 2 , ], 3)
# 
# head(TestFreqTable[ TestFreqTable$freq == 3 , ], 3)
# 
# head(TestFreqTable[ TestFreqTable$freq == 4 , ], 3)
```

These words are generally meaningless in English. 

It has been suggested in the literature that the counts of individuals words in a corpus generally follows a power law, i.e. that the frequency of a word is inversely proportional to its frequency rank. Thus, the second most frequent word occurs half as often the most frequent word, the third most frequent word occurs one third as often as the most frequent word, etc - see http://en.wikipedia.org/wiki/Power_law. 


However, a formal hypothesis has rejected this possibility (details are available on request). 
In the plot below, we have set out the logs of the variables in the "frequency count table". The straight line correspond to the best possible fit of a power law. This confirms that, for words that appear very frequently, the observations deviate substantially from the predictions of a power law. 



```{r, echo=FALSE, cache=TRUE}
# plot(FreqPow)
# lines(FreqPow)
# boot_res <- bootstrap_p(FreqPow, xmins = FreqPow$getXmin())

rm()
```


The next question we need to consider is the distribution of n-grams, i.e.  contiguous sequences of n items in the database. For the purposes of this project, we have constructed 2-grams, 3-grams and 4-grams. 

As a full discussion of the properties of all n-grams would take us too far, we focus here on the analysis of 2-grams.



```{r, echo=FALSE, cache=TRUE}
ngramunique <- read.table("US.twitterTokened2Gr_uniq.txt", stringsAsFactors = FALSE)
#ngramunique <- readLines("US.twitterTokened4Gr_uniq.txt")
names(ngramunique) <- c("freq", "n-gram")
nrow(ngramunique)
ngramunique <- ngramunique[complete.cases(ngramunique),]
nrow(ngramunique)

ngramunique <- ngramunique[order(ngramunique$freq, decreasing = TRUE), ]
ngramuniqueFreq <-  CutoffTable(ngramunique, "freq", 0.9)

min_fretable_ngr <- rbind(c(2, nrow(ngramunique[ngramunique$freq > 1, ])), c(3,nrow(ngramunique[ngramunique$freq > 2, ])))
min_fretable_ngr <- rbind(min_fretable_ngr, c(10, nrow(ngramunique[ngramunique$freq > 9, ])))
min_fretable_ngr <- rbind(min_fretable_ngr, c(100, nrow(ngramunique[ngramunique$freq > 99, ])))
min_fretable_ngr <- rbind(min_fretable_ngr, c(1000, nrow(ngramunique[ngramunique$freq > 999,])))
min_fretable_ngr <- rbind(min_fretable_ngr, c(10000, nrow(ngramunique[ngramunique$freq > 9999, ])))
min_fretable_ngr <- as.data.frame(min_fretable_ngr)
min_fretable_ngr$share <- min_fretable_ngr[, 2]/nrow(ngramunique) * 100
names(min_fretable_ngr) <- c("Minimum # of occurences", "# of unique 2-grams", "Share in total # of unique 2-grams")

```

The  Twitter table contains  a total of
`r sprintf("%1.1f", ngramuniqueFreq[[2]]/10^6) ` million 2-grams, with  `r sprintf("%1.0f", nrow(ngramunique)/1000) ` thousand unqiue 2-grams.

Out of these unique 2-grams, `r sprintf("%0d", nrow(ngramuniqueFreq[[3]])) ` 2-grams (this is thus `r sprintf("%1.2f", nrow(ngramuniqueFreq[[3]])/nrow(ngramunique) * 100) ` %) correspond to 90% of all 2-gram occurences.




