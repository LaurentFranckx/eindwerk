---
title: "Milestone report"
author: "Me"
date: "Thursday, March 19, 2015"
output: html_document
---

#Introduction

This paper presents intermediate results in the construction of a predictive text model. The final objective
of this model is to provide users for suggestions for autocompleting a sentence they have started. 

The training data set has been obtained from the Coursera website, but are originally  from a corpus called HC Corpora (www.corpora.heliohost.org). The complete set consists of databases in English, German, Russian and Finnish. In this project we will limit ourselves to the English database. 

The English database itself is composed of three files, each composed of several hundreds of thousands of text lines from blogs, news cites and Twitter. 

As a first step in the paper, we will  explain the approach used to the cleaning of the data. Next, we will summarize the main descriptive statistics for each dataset, and explore how these compare to each other. Finally, we will describe the next steps we intend to undertake in order to construct our model. 


#Data cleaning

As a first step in our project, we have cleaned the original data. 

- First, we have modified the encoding from "UTF-8" to "latin1" - not doing so would lead to the creation of unreadable characters.
- Second, we have removed any numbers present.
- Third we have removed all the words that are banned by Google for their profane or offensive character.
- Fourth, we have removed all white spaces that have resulted frm the previous steps. 

Howevern, punctuation and other non-alphanumeric characters are an integral part of the sentence (especially in the case of Twitter), as have therefore been kept as they were.  

Although foreign words have crept inside the database, we have left them as they were. Indeed, some foreign words are deliberately used in English to convey a specific message. If foreign sentences have been included inadvertently in the database, they are likely to be eliminated in the next step of the analysis, where we have eliminated all words that did not occur a minimum number of times. 



#Descriptive statistics for the unique words


```{r, echo=FALSE, cache=2}
library("poweRlaw")
library("plyr")
library("ggplot2")

CutoffTable <- function(df, freq_val, value){
  df$sum_of_freq <- cumsum(df[, freq_val])
  total_count <- sum(df[, freq_val])
  df$sum_of_shares <- df$sum_of_freq/total_count
  df_low <- df[df$sum_of_shares < value,   ]
  returnlist <- list(df, total_count, df_low)
  names(returnlist) <- c("OrDF", "Count","LowDF")
  return(returnlist)
}
setwd("D:/coursera/dsc_capstone")
folder_txt <- file.path("Coursera-SwiftKey","final","en_US")

corpusnames <- c("news","twitter","blogs")

or_tables <- list()
freq_tables <- list()
min_fretables <- list()

for(name in corpusnames){
  or_tables[[name]] <- readLines(file.path(folder_txt, paste("en_US", name, "txt", sep =".")))
  TestFreqTable <- read.table(paste("US.", name , "unique.txt", sep=""), stringsAsFactors = FALSE)
  names(TestFreqTable) <- c("freq", "word")
  min_fretable <- rbind(c(2, nrow(TestFreqTable[TestFreqTable$freq > 1, ])), c(10,nrow( TestFreqTable[TestFreqTable$freq > 9, ])))
  #min_fretable <- rbind(min_fretable, c(10, nrow(TestFreqTable[TestFreqTable$freq > 9, ])))
  min_fretable <- rbind(min_fretable, c(20, nrow(TestFreqTable[TestFreqTable$freq > 19, ])))
  min_fretable <- rbind(min_fretable, c(30, nrow(TestFreqTable[TestFreqTable$freq > 29,])))
  min_fretable <- rbind(min_fretable, c(40, nrow(TestFreqTable[TestFreqTable$freq > 39, ])))
  min_fretable <- rbind(min_fretable, c(50, nrow(TestFreqTable[TestFreqTable$freq > 49, ])))
#  min_fretable <- rbind(min_fretable, c(40, nrow(TestFreqTable[TestFreqTable$freq > 39, ])))
  min_fretable <- as.data.frame(min_fretable)
  min_fretable$share <- min_fretable[, 2]/nrow(TestFreqTable) * 100
  names(min_fretable) <- c("MinOccur", paste("Uniq", name,sep=""), paste("Share",name,sep=""))
  min_fretable <- as.data.frame(min_fretable)
  min_fretables[[name]] <- min_fretable
  freq_tables[[name]]  <- TestFreqTable
}

GlobFreqTable <- Reduce("join", min_fretables)
GlobFreqTable2 <- reshape(GlobFreqTable, idvar ="MinOccur", times = corpusnames , varying = paste("Share",corpusnames,sep="") , v.names = "Share", direction = "long")
qplot(MinOccur, data= GlobFreqTable2, geom = "bar", fill = time, weight = Share, position = "dodge", binwidth = 5) + xlab("Minimal occurence of word") + ylab("Count in corpus")


#names(TestFreqTable) <- c("freq", "word")
#GlobFreqTable <- GlobFreqTable[order(GlobFreqTable$freq, decreasing = TRUE), ]
TestFreqList <- lapply(freq_tables, CutoffTable, "freq", 0.9)

textlines <- c(sprintf("%1.1f", length(or_tables[["blogs"]])/10^6 ),sprintf("%1.1f", length(or_tables[["twitter"]])/10^6 ),sprintf("%1.1f", length(or_tables[["news"]])/10^6 ))
textlines <- as.numeric(textlines)

wordcount <- c(sprintf("%1.1f", TestFreqList[["blogs"]]$Count/10^6 ),sprintf("%1.1f", TestFreqList[["twitter"]]$Count/10^6 ),sprintf("%1.1f", TestFreqList[["news"]]$Count/10^6 ))
wordcount <- as.numeric(wordcount)

uniquewords <- c(sprintf("%1.1f", nrow(TestFreqList[["blogs"]]$OrDF)/10^3 ),sprintf("%1.1f", nrow(TestFreqList[["twitter"]]$OrDF)/10^3 ),sprintf("%1.1f", nrow(TestFreqList[["news"]]$OrDF)/10^3 ))
uniquewords <- as.numeric(uniquewords)

highshare <- c(sprintf("%1.0f", nrow(TestFreqList[["blogs"]]$LowDF)/10^3 ),sprintf("%1.0f", nrow(TestFreqList[["twitter"]]$LowDF)/10^3 ),sprintf("%1.0f", nrow(TestFreqList[["news"]]$LowDF)/10^3 ))
highshare <- as.numeric(highshare)

SummarTableWords <- cbind(textlines,wordcount)
SummarTableWords <- cbind(SummarTableWords,uniquewords)
SummarTableWords <- cbind(SummarTableWords,highshare)
SummarTableWords <- as.data.frame(SummarTableWords)
SummarTableWords$highsharrel <-  with(SummarTableWords,sprintf("%0.2f",highshare/(wordcount *10^3) *100))
row.names(SummarTableWords) <- c("blogs","twitter","news")
SummarTableWords <- SummarTableWords[, names(SummarTableWords) != "highshare"]
names(SummarTableWords) <- c("Lines of text (10^6)", "Word count (10^6)" , "Unique words (10^3)", "Perc90")


knitr::kable(SummarTableWords, caption = "Key summary statistics")

#GlobFreqTable$logMinOccur <- log10(GlobFreqTable$MinOccur)

```

The table above gives some key information on "blogs", "Twitter"" and "news" databases

- the first column gives, for the "uncleaned" version of the databases, the number of lines of text
- the second column gives, for the databases restricted to alphanumeric characters, the total number of words
- the third column gives, for the databases restricted to alphanumeric characters, the number of unique words
- the fourth column gives (in %), the number of unique words corresponding to 90% of all word occurences. 

The general skewness of the frequency of words is confirmed when we look at the summary statistics of the number of counts:  

```{r, echo=FALSE}
knitr::kable(    sapply(freq_tables, function(x) summary(x[, "freq"])), caption = "Summary statistics for word counts")
rm(or_tables)
gc()
```



Still another way to look at the data is to calculate how many individual words appear in the database with a given count. We represent here the first five and the last five lines of the table containing this information (henceforth the "frequency count table"). 



```{r, echo=FALSE, cache=2}


CreateCountTable <- function(corpus){
  freqtable <- freq_tables[[corpus]]
  freq_count <- freqtable$freq
  FreqPow <- displ$new(freq_count)
#FreqPow$getXmin()
#USBlogFreqPowPars <- estimate_pars(USBlogFreqPow, pars = NULL)
  FreqPowMin <- estimate_xmin(FreqPow, pars = NULL)
  FreqPow$setXmin(FreqPowMin)
  FrqvsVal <- cbind(FreqPow$internal$freq,FreqPow$internal$values)
  FrqvsVal <- as.data.frame(FrqvsVal)
  names(FrqvsVal ) <- c(paste("# of individuals words", corpus), paste("# of counts in", corpus))
  return(FrqvsVal) 
}

CounttableTwit <- CreateCountTable("twitter")
Counttableblogs <- CreateCountTable("blogs")
Counttablenews <- CreateCountTable("news")


```

```{r, echo=FALSE, cache=2}
head_table <- cbind(head(CounttableTwit),head(Counttableblogs))
head_table <- cbind(head_table,head(Counttablenews))
head_table

tail_table <- cbind(tail(CounttableTwit),tail(Counttableblogs))
tail_table <- cbind(tail_table,tail(Counttablenews))
tail_table



#most_freq_words <- sapply(corpusnames, function(x)   freq_tables[[x]][ freq_tables[[x]]$freq == max(freq_tables[[x]]$freq) , "word" ])
most_freq_words <- sapply(corpusnames, function(x)   freq_tables[[x]][ freq_tables[[x]]$freq %in% tail_table[, paste(paste("# of counts in", x)) ] , "word" ])


```

Thus, for instance, `r sprintf("%1.0f",head_table[1, 1]/1000)` thousand individual words appear exactly `r head_table[1, 2]` time in the Twitter database, while `r head_table[nrow(head_table), 1]` word  appears exactly `r head_table[nrow(head_table), 2]` times in the Twitter database.

For the three databases, the 6 most frequent words, are:

```{r, echo=FALSE, cache=2}
knitr::kable(most_freq_words, caption= "Most frequent words in the three databases")
```

Finally, we can have a look at a sample of words that appear very infrequently in the database (here, less than 5 times): 

```{r, echo=FALSE, cache=2}
set.seed(123)

VeryInfWords <- function(corpus, lessthan = 5, samplesize = 10){
  freqtable <- freq_tables[[corpus]]
  low_frq <- TestFreqTable[ TestFreqTable$freq < lessthan , ]
  low_frq <- low_frq[sample(1:nrow(low_frq), samplesize)  , ]
  names(low_frq) <- c(paste("freq in", corpus), paste("word from", corpus))
  return(low_frq)
}

fewwords <- lapply(corpusnames, VeryInfWords)
do.call("cbind",fewwords)

# head(TestFreqTable[ TestFreqTable$freq == min(TestFreqTable$freq) , ], 3)
# 
# head(TestFreqTable[ TestFreqTable$freq == 2 , ], 3)
# 
# head(TestFreqTable[ TestFreqTable$freq == 3 , ], 3)
# 
# head(TestFreqTable[ TestFreqTable$freq == 4 , ], 3)
```

These words are generally meaningless in English, or are obvious spelling mistakes.  

It has been suggested in the literature that the counts of individuals words in a corpus generally follows a power law, i.e. that the frequency of a word is inversely proportional to its frequency rank. Thus, the second most frequent word occurs half as often the most frequent word, the third most frequent word occurs one third as often as the most frequent word, etc - see http://en.wikipedia.org/wiki/Power_law. 


However, a formal hypothesis has rejected this possibility for the databases we use here (details are available on request). 
In the plot below, we have set out the logs of the variables in the "frequency count table". The straight line correspond to the best possible fit of a power law. This confirms that, for words that appear very frequently, the observations deviate substantially from the predictions of a power law. 



```{r, echo=FALSE, cache=2}
# plot(FreqPow)
# lines(FreqPow)
# boot_res <- bootstrap_p(FreqPow, xmins = FreqPow$getXmin())

rm()
```


The next question we need to consider is the distribution of n-grams, i.e.  contiguous sequences of n items in the database. For the purposes of this project, we have constructed 2-grams, 3-grams and 4-grams. 

As a full discussion of the properties of all n-grams would take us too far, we focus here on the analysis of 2-grams.



```{r, echo=FALSE, cache=2}
#ngramunique <- read.table("US.twitterTokened2Gr_uniq.txt", stringsAsFactors = FALSE)
#ngramunique <- readLines("US.twitterTokened4Gr_uniq.txt")
or_tables2Gr <- list()
freq_tables2Gr <- list()
min_fretables2Gr <- list()

for(name in corpusnames){
#  or_tables2Gr[[name]] <- readLines(file.path(folder_txt, paste("en_US", name, "txt", sep =".")))
  ngramunique <- read.table(paste("US.", name , "Tokened2Gr_uniq.txt", sep=""), stringsAsFactors = FALSE)
  names(ngramunique) <- c("freq", "n-gram")
  ngramunique <- ngramunique[complete.cases(ngramunique),]
  min_fretable <- rbind(c(2, nrow(ngramunique[ngramunique$freq > 1, ])), c(10,nrow( ngramunique[ngramunique$freq > 9, ])))
  #min_fretable <- rbind(min_fretable, c(10, nrow(ngramunique[ngramunique$freq > 9, ])))
  min_fretable <- rbind(min_fretable, c(20, nrow(ngramunique[ngramunique$freq > 19, ])))
  min_fretable <- rbind(min_fretable, c(30, nrow(ngramunique[ngramunique$freq > 29,])))
  min_fretable <- rbind(min_fretable, c(40, nrow(ngramunique[ngramunique$freq > 39, ])))
  min_fretable <- rbind(min_fretable, c(50, nrow(ngramunique[ngramunique$freq > 49, ])))
#  min_fretable <- rbind(min_fretable, c(40, nrow(ngramunique[ngramunique$freq > 39, ])))
  min_fretable <- as.data.frame(min_fretable)
  min_fretable$share <- min_fretable[, 2]/nrow(ngramunique) * 100
  names(min_fretable) <- c("MinOccur", paste("Uniq", name,sep=""), paste("Share",name,sep=""))
  min_fretable <- as.data.frame(min_fretable)
  min_fretables2Gr[[name]] <- min_fretable
  freq_tables2Gr[[name]]  <- ngramunique
}

GlobFreqTable2Gr <- Reduce("join", min_fretables2Gr)
GlobFreqTable2Gr2  <- reshape(GlobFreqTable2Gr, idvar ="MinOccur", times = corpusnames , varying = paste("Share",corpusnames,sep="") , v.names = "Share", direction = "long")
qplot(MinOccur, data= GlobFreqTable2Gr2, geom = "bar", fill = time, weight = Share, position = "dodge", binwidth = 5) + xlab("Minimal occurence of 2-gram") + ylab("Count in corpus")

TestFreqList2Gr <- lapply(freq_tables2Gr, CutoffTable, "freq", 0.9)

TwoGrcount <- c(sprintf("%1.1f", TestFreqList2Gr[["blogs"]]$Count/10^6 ),sprintf("%1.1f", TestFreqList2Gr[["twitter"]]$Count/10^6 ),sprintf("%1.1f", TestFreqList2Gr[["news"]]$Count/10^6 ))
wordcount <- as.numeric(TwoGrcount)

TwoGruniquewords <- c(sprintf("%1.1f", nrow(TestFreqList2Gr[["blogs"]]$OrDF)/10^3 ),sprintf("%1.1f", nrow(TestFreqList2Gr[["twitter"]]$OrDF)/10^3 ),sprintf("%1.1f", nrow(TestFreqList2Gr[["news"]]$OrDF)/10^3 ))
uniquewords <- as.numeric(TwoGruniquewords)

TwoGrhighshare <- c(sprintf("%1.0f", nrow(TestFreqList2Gr[["blogs"]]$LowDF)/10^3 ),sprintf("%1.0f", nrow(TestFreqList2Gr[["twitter"]]$LowDF)/10^3 ),sprintf("%1.0f", nrow(TestFreqList2Gr[["news"]]$LowDF)/10^3 ))
highshare <- as.numeric(TwoGrhighshare)


SummarTable2Gr <- cbind(TwoGrcount,TwoGruniquewords)
SummarTable2Gr <- cbind(SummarTable2Gr,TwoGrhighshare)
SummarTable2Gr <- as.data.frame(SummarTable2Gr)
SummarTable2Gr$highsharrel <-  with(SummarTable2Gr,sprintf("%0.2f",highshare/(wordcount *10^3) *100))
row.names(SummarTable2Gr) <- c("blogs","twitter","news")
SummarTable2Gr <- SummarTable2Gr[, names(SummarTable2Gr) != "TwoGrhighshare"]
names(SummarTable2Gr) <- c( "2-gram count (10^6)" , "Unique 2-gram (10^3)", "Perc90")

#ngramunique <- ngramunique[order(ngramunique$freq, decreasing = TRUE), ]
#ngramuniqueFreq <-  CutoffTable(ngramunique, "freq", 0.9)
# min_fretable_ngr$share <- min_fretable_ngr[, 2]/nrow(ngramunique) * 100
# names(min_fretable_ngr) <- c("Minimum # of occurences", "# of unique 2-grams", "Share in total # of unique 2-grams")

```

The  Twitter table contains  a total of
`r sprintf("%1.1f", ngramuniqueFreq[[2]]/10^6) ` million 2-grams, with  `r sprintf("%1.0f", nrow(ngramunique)/1000) ` thousand unqiue 2-grams.

Out of these unique 2-grams, `r sprintf("%0d", nrow(ngramuniqueFreq[[3]])) ` 2-grams (this is thus `r sprintf("%1.2f", nrow(ngramuniqueFreq[[3]])/nrow(ngramunique) * 100) ` %) correspond to 90% of all 2-gram occurences.

Another indication of the relative frequency with which 2-grams occur is to compare how many 2-grams 
appear a minimum number of times in the whole database: 

```{r, echo=FALSE}
#summary(TestFreqTable$freq)
min_fretable_ngr

```

Thus, `r nrow(ngramunique[ngramunique$freq > 1, ])` individual 2-grams appear more than once in the database,
`r nrow(ngramunique[ngramunique$freq > 2, ])` individual 2-grams appear more than twice in the database, etc.

Another indication of the  skewness of the frequency of 2-grams is obtained when we look at the summary statistics of the number of counts: 

```{r, echo=FALSE}
summary(ngramunique$freq)
```


Still another way to look at the data is to calculate how many individual 2-grams with a given count appear in the database. We represent here the first five and the last five lines of the table containing this information (henceforth the "frequency count table"). 

```{r, echo=FALSE, cache=2}
# freq_count_ngr  <- ngramunique$freq
# FreqPow2Gr <- displ$new(freq_count_ngr)
# #FreqPow2Gr$getXmin()
# #USBlogFreqPow2GrPars <- estimate_pars(USBlogFreqPow2Gr, pars = NULL)
# FreqPow2GrMin <- estimate_xmin(FreqPow2Gr, pars = NULL)
# FreqPow2Gr$setXmin(FreqPow2GrMin)
# FrqvsVal2Gr <- cbind(FreqPow2Gr$internal$freq,FreqPow2Gr$internal$values)
# FrqvsVal2Gr <- data.frame(FrqvsVal2Gr)
# names(FrqvsVal2Gr ) <- c("Number of individuals 2-grams", "Number of counts in the whole database")
```

```{r, echo=FALSE, cache=2}
# head(FrqvsVal2Gr )
# tail(FrqvsVal2Gr )
# most_freq_2gr <- ngramunique[ ngramunique$freq == max(ngramunique$freq) , "n-gram" ]
# most_freq_2gr 
```

Thus `r sprintf("%1.0f", FrqvsVal2Gr[1, 1]/1000)` thousand individual 2-grams appear exactly `r FrqvsVal2Gr[1, 2]` time in the database, while `r FrqvsVal2Gr[nrow(FrqvsVal2Gr), 1]` 2-gram (this is " `r most_freq_2gr` ""), appears exactly `r FrqvsVal2Gr[nrow(FrqvsVal2Gr), 2]` times in the database.


Finally, we can have a look at a sample of 2-grams that appear very infrequently in the database: 

```{r, echo=FALSE, cache=2}
set.seed(123)
low_frq <- ngramunique[ ngramunique$freq < 7 , ]
low_frq[sample(1:nrow(low_frq), 10)  , ]
```

We see that there are still a lot of these 2-grams that make little sense in English, but compared to individual words, there are also some "low frequency" 2-grams that do make sense.


Finally, we consider the possibility that the 2-grams may follow a power law. 


```{r, echo=FALSE, cache=2}
# plot(FreqPow2Gr)
# lines(FreqPow2Gr)
# boot_res <- bootstrap_p(FreqPow2Gr, xmins = FreqPow2Gr$getXmin())
rm()
```


