---
title: "Milestone report"
author: "Me"
date: "Thursday, March 19, 2015"
output: pdf_document
---

#Introduction

This paper presents intermediate results in the construction of a predictive text model. The final objective
of this model is to provide users for suggestions for autocompleting a sentence they have started. 

The training data set has been obtained from the Coursera website, but are originally  from a corpus called HC Corpora (www.corpora.heliohost.org). The complete set consists of databases in English, German, Russian and Finnish. In this project we will limit ourselves to the English database. 

The English database itself is composed of three files, each composed of several hundreds of thousands of text lines from blogs, news cites and Twitter. 

As a first step in the paper, we will  explain the approach used to the cleaning of the data. Next, we will summarize the main descriptive statistics for each dataset, and explore how these compare to each other. Finally, we will describe the next steps we intend to undertake in order to construct our model. 


#Data cleaning

As a first step in our project, we have cleaned the original data. 

- First, we have modified the encoding from "UTF-8" to "latin1" - not doing so would lead to the creation of unreadable characters.
- Second, we have removed any numbers present.
- Third we have removed all the words that are banned by Google for their profane or offensive character.
- Fourth, we have removed all white spaces that have resulted frm the previous steps. 

Howevern, punctuation and other non-alphanumeric characters are an integral part of the sentence (especially in the case of Twitter), as have therefore been kept as they were.  

Although foreign words have crept inside the database, we have left them as they were. Indeed, some foreign words are deliberately used in English to convey a specific message. If foreign sentences have been included inadvertently in the database, they are likely to be eliminated in the next step of the analysis, where we have eliminated all words that did not occur a minimum number of times. 



#Descriptive statistics of the twitter file


```{r, echo=FALSE, cache=TRUE}
library("poweRlaw")

CutoffTable <- function(df, freq_val, value){
  df$sum_of_freq <- cumsum(df[, freq_val])
  total_count <- sum(df[, freq_val])
  df$sum_of_shares <- df$sum_of_freq/total_count
  df_low <- df[df$sum_of_shares < value,   ]
  return(list(df, total_count, df_low))
}
setwd("D:/coursera/dsc_capstone")
folder_txt <- file.path("Coursera-SwiftKey","final","en_US")
or_table <- readLines(file.path(folder_txt, "en_US.twitter.txt"))
TestFreqTable <- read.table("US.twitterunique.txt", stringsAsFactors = FALSE)
names(TestFreqTable) <- c("freq", "word")
TestFreqTable <- TestFreqTable[order(TestFreqTable$freq, decreasing = TRUE), ]
TestFreqList <- CutoffTable(TestFreqTable, "freq", 0.9)

min_fretable <- rbind(c(1, nrow(TestFreqTable[TestFreqTable$freq > 1, ])), c(2,nrow( TestFreqTable[TestFreqTable$freq > 2, ])))
min_fretable <- rbind(min_fretable, c(9, nrow(TestFreqTable[TestFreqTable$freq > 9, ])))
min_fretable <- rbind(min_fretable, c(99, nrow(TestFreqTable[TestFreqTable$freq > 99, ])))
min_fretable <- rbind(min_fretable, c(999, nrow(TestFreqTable[TestFreqTable$freq > 999,])))
min_fretable <- rbind(min_fretable, c(9999, nrow(TestFreqTable[TestFreqTable$freq > 9999, ])))
min_fretable <- as.data.frame(min_fretable)
names(min_fretable) <- c("Minimum number of occurences", "Number of unique words")


```

The "uncleaned" version of the Twitter table contains `r sprintf("%1.1f", length(or_table)/10^6)` millions lines of text. 

When we limit the table to alphanumeric characters, it contains a total of
`r sprintf("%1.0f", TestFreqList[[2]]/10^6) ` million occurences of words, with  `r sprintf("%1.0f", nrow(TestFreqTable)/1000) ` thousand unique words.

Out of these unique words, `r sprintf("%0d", nrow(TestFreqList[[3]])) ` words (this is thus `r sprintf("%1.2f", nrow(TestFreqList[[3]])/nrow(TestFreqTable) * 100) ` %) correspond to 90% of all word occurences.

Another indication of the relative frequency with which words occur is to compare how many words 
appear a minimum number of times in the whole database: 

```{r}
#summary(TestFreqTable$freq)

min_fretable

```

Thus, `r nrow(TestFreqTable[TestFreqTable$freq > 1, ])` individual words appear more than once in the database,
`r nrow(TestFreqTable[TestFreqTable$freq > 2, ])` individual words appear more than twice in the database, etc.

The general skewness of the frequency of words is confirmed when we look at the summary statistics of the number of counts: 

```{r}
summary(TestFreqTable$freq)
```

It has been suggested that the counts of individuals words in a corpus generally follows a power law.


```{r, echo=FALSE, cache=TRUE}
freq_count <- TestFreqTable$freq
FreqPow <- displ$new(freq_count)
#FreqPow$getXmin()
#USBlogFreqPowPars <- estimate_pars(USBlogFreqPow, pars = NULL)
FreqPowMin <- estimate_xmin(FreqPow, pars = NULL)
FreqPow$setXmin(FreqPowMin)
df <- cbind(FreqPow$internal$freq,FreqPow$internal$values)
head(df)
tail(df)
```

Therefore, we calculate 

```{r, echo=FALSE, cache=TRUE}
# plot(FreqPow)
# lines(FreqPow)
# boot_res <- bootstrap_p(FreqPow, xmins = FreqPow$getXmin())
```


