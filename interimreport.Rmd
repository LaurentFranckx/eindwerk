---
title: "Milestone report"
author: "Me"
date: "Thursday, March 19, 2015"
output: html_document
---
#Main text

##Introduction

This paper presents intermediate results in the construction of a predictive text model. The final objective
of this project is to construct a Shiny app which provides users with suggestions for auto completing a sentence they have started. 

The training data set has been obtained from the Coursera website, but are originally  from a corpus called HC Corpora (www.corpora.heliohost.org). The complete set consists of databases in English, German, Russian and Finnish. In this project we will limit ourselves to the English database. The English database itself is composed of three files, each composed of data sets containing several hundreds of thousands of text lines from blogs, news sites and Twitter. 

As a first step, we will summarize the main descriptive statistics for each dataset, and explore how these compare to each other - these statistics will be provided for individual words and for 2-grams (for a definition of this term, see further). 
As a next step, we will  explain the approach used to the cleaning  and the organisation of the data.
Finally, we will describe the next steps we intend to undertake in order to construct our predictive model. 




##Descriptive statistics for the unique words

Before undertaking any concrete modelling steps, it is important to understand the main features of the data. As we shall see below, our understanding of the data will allow us to drastically save on computer resources without jeopardizing predictive accuracy.

On prior grounds, one would expect the three datasets to differ on some important points. For instance, Twitter can be expected to contain relatively more abbreviations, relatively few stopwords, and more non-alphanumerical symbols than the other datasets. We would also expect Twitter to contain less grammatically correct sentences. The differences between the "blogs" and the "news" datasets are probably less pronounced, but one would expect the "news" dataset to use a more formal vocabulary, and to contain less spelling and grammatical mistakes. Therefore, we will analyse the key statistics separately.  


```{r, echo=FALSE, cache=2, message=FALSE, warning=FALSE}
library("poweRlaw")
library("plyr")
library("ggplot2")

CutoffTable <- function(df, freq_val, value){
  df$sum_of_freq <- cumsum(df[, freq_val])
  total_count <- sum(df[, freq_val])
  df$sum_of_shares <- df$sum_of_freq/total_count
  df_low <- df[df$sum_of_shares < value,   ]
  returnlist <- list(df, total_count, df_low)
  names(returnlist) <- c("OrDF", "Count","LowDF")
  return(returnlist)
}
setwd("D:/coursera/dsc_capstone")
folder_txt <- file.path("Coursera-SwiftKey","final","en_US")

corpusnames <- c("news","twitter","blogs")

or_tables <- list()
freq_tables <- list()
min_fretables <- list()

for(name in corpusnames){
  or_tables[[name]] <- readLines(file.path(folder_txt, paste("en_US", name, "txt", sep =".")))
  TestFreqTable <- read.table(paste("US.", name , "unique.txt", sep=""), stringsAsFactors = FALSE)
  names(TestFreqTable) <- c("freq", "word")
  min_fretable <- rbind(c(1, nrow(TestFreqTable[TestFreqTable$freq > 0, ])), c(10,nrow( TestFreqTable[TestFreqTable$freq > 9, ])))
  #min_fretable <- rbind(min_fretable, c(10, nrow(TestFreqTable[TestFreqTable$freq > 9, ])))
  min_fretable <- rbind(min_fretable, c(20, nrow(TestFreqTable[TestFreqTable$freq > 19, ])))
  min_fretable <- rbind(min_fretable, c(30, nrow(TestFreqTable[TestFreqTable$freq > 29,])))
  min_fretable <- rbind(min_fretable, c(40, nrow(TestFreqTable[TestFreqTable$freq > 39, ])))
  min_fretable <- rbind(min_fretable, c(50, nrow(TestFreqTable[TestFreqTable$freq > 49, ])))
#  min_fretable <- rbind(min_fretable, c(40, nrow(TestFreqTable[TestFreqTable$freq > 39, ])))
  min_fretable <- as.data.frame(min_fretable)
  min_fretable$share <- min_fretable[, 2]/nrow(TestFreqTable) * 100
  names(min_fretable) <- c("MinOccur", paste("Uniq", name,sep=""), paste("Share",name,sep=""))
  min_fretable <- as.data.frame(min_fretable)
  min_fretables[[name]] <- min_fretable
  freq_tables[[name]]  <- TestFreqTable
}

GlobFreqTable <- Reduce("join", min_fretables)
GlobFreqTable2 <- reshape(GlobFreqTable, idvar ="MinOccur", times = corpusnames , varying = paste("Share",corpusnames,sep="") , v.names = "Share", direction = "long")



#names(TestFreqTable) <- c("freq", "word")
#GlobFreqTable <- GlobFreqTable[order(GlobFreqTable$freq, decreasing = TRUE), ]
TestFreqList <- lapply(freq_tables, CutoffTable, "freq", 0.9)

textlines <- c(sprintf("%1.1f", length(or_tables[["blogs"]])/10^6 ),sprintf("%1.1f", length(or_tables[["twitter"]])/10^6 ),sprintf("%1.1f", length(or_tables[["news"]])/10^6 ))
textlines <- as.numeric(textlines)

wordcount <- c(sprintf("%1.1f", TestFreqList[["blogs"]]$Count/10^6 ),sprintf("%1.1f", TestFreqList[["twitter"]]$Count/10^6 ),sprintf("%1.1f", TestFreqList[["news"]]$Count/10^6 ))
wordcount <- as.numeric(wordcount)

uniquewords <- c(sprintf("%1.1f", nrow(TestFreqList[["blogs"]]$OrDF)/10^3 ),sprintf("%1.1f", nrow(TestFreqList[["twitter"]]$OrDF)/10^3 ),sprintf("%1.1f", nrow(TestFreqList[["news"]]$OrDF)/10^3 ))
uniquewords <- as.numeric(uniquewords)

highshare <- c(sprintf("%1.0f", nrow(TestFreqList[["blogs"]]$LowDF)/10^3 ),sprintf("%1.0f", nrow(TestFreqList[["twitter"]]$LowDF)/10^3 ),sprintf("%1.0f", nrow(TestFreqList[["news"]]$LowDF)/10^3 ))
highshare <- as.numeric(highshare)

SummarTableWords <- cbind(textlines,wordcount)
SummarTableWords <- cbind(SummarTableWords,uniquewords)
SummarTableWords <- cbind(SummarTableWords,highshare)
SummarTableWords <- as.data.frame(SummarTableWords)
SummarTableWords$highsharrel <-  with(SummarTableWords,sprintf("%0.2f",highshare/(wordcount *10^3) *100))
row.names(SummarTableWords) <- c("blogs","twitter","news")
SummarTableWords <- SummarTableWords[, names(SummarTableWords) != "highshare"]
names(SummarTableWords) <- c("Lines of text (10^6)", "Word count (10^6)" , "Unique words (10^3)", "Perc90")


knitr::kable(SummarTableWords, caption = "Key summary statistics for unique words")

#GlobFreqTable$logMinOccur <- log10(GlobFreqTable$MinOccur)

```

The table above gives some key information on the "blogs", "Twitter"" and "news" datasets:

- the first column gives, for the "uncleaned" version of the databases, the number of lines of text in millions;
- the second column gives, for the databases restricted to alphanumeric characters, the total number of words in millions;
- the third column gives, for the databases restricted to alphanumeric characters, the number of *unique* words in thousands;
- the fourth column gives (in %), the number of *unique* words corresponding to 90% of *all* word occurrences. 

We notice that while the Twitter dataset contains approximately three times more lines of text than the other datasets, it contains less words, which is due to the restrictions imposed on the length of individual tweets. On the other hand, the Twitter dataset contains more *unique* words, reflecting probably idiosyncratic expressions and abbreviations used by the "tweeters". 

We also observe that, in all three datasets, the vast majority of word occurrences originates from a very small (data set specific) dictionary. The general skewness of the frequency of words is confirmed when we look at the summary statistics of the  counts of *unique*  words (see Appendix): these statistics confirm that the vast majority of unique words appears just a few times in the total database, but that there are some words that occur with a very high frequency. This is also illustrated by the plot in Appendix, which shows how the share of individual words in the set of *unique* words changes when we require that they appear at least, respectively, 1, 10, 20, 30, 40 or 50 times in the respective complete dataset. We see a sharp initial decrease in this share. For instance, in all three datasets, the share of words occurring at least ten times is between (approximately) 15 and 25% of all unique words. 


Still another way to look at the data is to ask: how many *unique* words appear exactly 1, 2, 3 etc times? In Appendix, we represent  the first five and the last five lines of the table containing this information (henceforth the "frequency count tables"). 



```{r, echo=FALSE, cache=2}


CreateCountTable <- function(tablelist, corpus){
  freqtable <- tablelist[[corpus]]
  freq_count <- freqtable$freq
  FreqPow <- displ$new(freq_count)
#FreqPow$getXmin()
#USBlogFreqPowPars <- estimate_pars(USBlogFreqPow, pars = NULL)

  #test: how does this impact on model performance
#   FreqPowMin <- estimate_xmin(FreqPow, pars = NULL)
#   FreqPow$setXmin(FreqPowMin)


  FrqvsVal <- cbind(FreqPow$internal$freq,FreqPow$internal$values)
  FrqvsVal <- as.data.frame(FrqvsVal)
  names(FrqvsVal ) <- c(paste("# of words/n-grams", corpus), paste("# of counts in", corpus))
  return(FrqvsVal) 
}

CounttableTwit <- CreateCountTable(freq_tables, "twitter")
Counttableblogs <- CreateCountTable(freq_tables, "blogs")
Counttablenews <- CreateCountTable(freq_tables, "news")


```

```{r, echo=FALSE, cache=2}
head_table <- cbind(head(CounttableTwit),head(Counttableblogs))
head_table <- cbind(head_table,head(Counttablenews))

tail_table <- cbind(tail(CounttableTwit),tail(Counttableblogs))
tail_table <- cbind(tail_table,tail(Counttablenews))

twit_one <- nrow(CounttableTwit[CounttableTwit[, "# of words/n-grams twitter"] == 1, 0])
twit_two <- nrow(CounttableTwit[CounttableTwit[ , "# of words/n-grams twitter"] == 2, 0])


#most_freq_words <- sapply(corpusnames, function(x)   freq_tables[[x]][ freq_tables[[x]]$freq == max(freq_tables[[x]]$freq) , "word" ])
most_freq_words <- sapply(corpusnames, function(x)   freq_tables[[x]][ freq_tables[[x]]$freq %in% tail_table[, paste("# of counts in", x) ] , "word" ])

head_table <- head_table[ , c(1,3,5,6)]
names(head_table) <- c("# of unique words in twitter", "# of unique words in blogs", "# of unique words in news", " counts")


row.names(tail_table) <- NULL 
tail_table <- tail_table[ , c(1,2,4,6)]
names(tail_table) <- c("# of unique words", "counts in twitter", "counts in blogs ", "counts in news " )


```

These tables show that, for instance, `r sprintf("%1.0f",head_table[1, 1]/1000)` thousand individual words appear exactly `r head_table[1, 4]` time in the Twitter database, while `r tail_table[nrow(tail_table), 1]` word  appears exactly `r sprintf("%1.0f", tail_table[nrow(tail_table), 2])` times in the Twitter database.

A  table in Appendix also gives the 6 most frequent words for each of the three datasets. 
None of the results are surprising, though it should be noted that the appearance of "you" and "i" in the Twitter top 6 and of "i" in the blogs top 6 are an indication of the more "personal" contents of these datasets. 




```{r, echo=FALSE}
set.seed(123)

VeryInfWords <- function( corpus, tables, lessthan = 5, samplesize = 10){
  freqtable <- tables[[corpus]]
  low_frq <- freqtable[ freqtable$freq < lessthan , ]
  low_frq <- low_frq[sample(1:nrow(low_frq), samplesize)  , ]
  names(low_frq) <- c("count ",  corpus)
  return(low_frq)
}

fewwords <- lapply(corpusnames, VeryInfWords, tables = freq_tables)
wordsample <- do.call("cbind",fewwords)
row.names(wordsample) <- NULL 


# head(TestFreqTable[ TestFreqTable$freq == min(TestFreqTable$freq) , ], 3)
# 
# head(TestFreqTable[ TestFreqTable$freq == 2 , ], 3)
# 
# head(TestFreqTable[ TestFreqTable$freq == 3 , ], 3)
# 
# head(TestFreqTable[ TestFreqTable$freq == 4 , ], 3)
```

Finally, we can have a look at a random sample of words that appear very infrequently in the database (here, less than 5 times) - see the table in Appendix for a sample of ten words. These words are generally meaningless in English, or are obvious spelling mistakes.  

It has been suggested in the literature that the counts of individuals words in a corpus generally follows a power law, i.e. that the frequency of a word is inversely proportional to its frequency rank. Thus, the second most frequent word occurs half as often the most frequent word, the third most frequent word occurs one third as often as the most frequent word, etc - see http://en.wikipedia.org/wiki/Power_law. However, a formal hypothesis has rejected this possibility for the databases we use here (details are available on request). This is confirmed by a visual analysis of the plots of the  "frequency count table" - see the Appendix for details. 

Nevertheless, from the overall analysis in this section, we can be confident that leaving out words that do not occur with a minimal frequency in each dataset is unlikely to affect the predictive performance of the model. We will discuss below how we have determined this "minimal" frequency.


##Descriptive statistics for 2-grams

The next question we need to consider is the distribution of n-grams, i.e.  contiguous sequences of n items in the database. As we shall discuss below, they will be the cornerstone of the predictive model. For the purposes of this project, we have constructed 2-grams, 3-grams and 4-grams. As a full discussion of the properties of all n-grams would take us beyond the scope of this paper, we focus here on the analysis of 2-grams. A full analysis is available on request. 

As in the analysis of individual words, we have performed a separate analysis for the "blogs", "Twitter"" and "news" datasets. 



```{r, echo=FALSE, cache=2, message=FALSE}
#ngramunique <- read.table("US.twitterTokened2Gr_uniq.txt", stringsAsFactors = FALSE)
#ngramunique <- readLines("US.twitterTokened4Gr_uniq.txt")
or_tables2Gr <- list()
freq_tables2Gr <- list()
min_fretables2Gr <- list()

for(name in corpusnames){
#  or_tables2Gr[[name]] <- readLines(file.path(folder_txt, paste("en_US", name, "txt", sep =".")))
  ngramunique <- read.table(paste("US.", name , "Tokened2Gr_uniq.txt", sep=""), stringsAsFactors = FALSE)
  names(ngramunique) <- c("freq", "n-gram")
  ngramunique <- ngramunique[complete.cases(ngramunique),]
  min_fretable <- rbind(c(1, nrow(ngramunique[ngramunique$freq > 0, ])), c(10,nrow( ngramunique[ngramunique$freq > 9, ])))
  #min_fretable <- rbind(min_fretable, c(10, nrow(ngramunique[ngramunique$freq > 9, ])))
  min_fretable <- rbind(min_fretable, c(20, nrow(ngramunique[ngramunique$freq > 19, ])))
  min_fretable <- rbind(min_fretable, c(30, nrow(ngramunique[ngramunique$freq > 29,])))
  min_fretable <- rbind(min_fretable, c(40, nrow(ngramunique[ngramunique$freq > 39, ])))
  min_fretable <- rbind(min_fretable, c(50, nrow(ngramunique[ngramunique$freq > 49, ])))
#  min_fretable <- rbind(min_fretable, c(40, nrow(ngramunique[ngramunique$freq > 39, ])))
  min_fretable <- as.data.frame(min_fretable)
  min_fretable$share <- min_fretable[, 2]/nrow(ngramunique) * 100
  names(min_fretable) <- c("MinOccur", paste("Uniq", name,sep=""), paste("Share",name,sep=""))
  min_fretable <- as.data.frame(min_fretable)
  min_fretables2Gr[[name]] <- min_fretable
  freq_tables2Gr[[name]]  <- ngramunique
}

GlobFreqTable2Gr <- Reduce("join", min_fretables2Gr)
GlobFreqTable2Gr2  <- reshape(GlobFreqTable2Gr, idvar ="MinOccur", times = corpusnames , varying = paste("Share",corpusnames,sep="") , v.names = "Share", direction = "long")


TestFreqList2Gr <- lapply(freq_tables2Gr, CutoffTable, "freq", 0.9)

TwoGrcount <- c(sprintf("%1.1f", TestFreqList2Gr[["blogs"]]$Count/10^6 ),sprintf("%1.1f", TestFreqList2Gr[["twitter"]]$Count/10^6 ),sprintf("%1.1f", TestFreqList2Gr[["news"]]$Count/10^6 ))
TwoGrcount <- as.numeric(TwoGrcount)

TwoGrunique <- c(sprintf("%1.1f", nrow(TestFreqList2Gr[["blogs"]]$OrDF)/10^3 ),sprintf("%1.1f", nrow(TestFreqList2Gr[["twitter"]]$OrDF)/10^3 ),sprintf("%1.1f", nrow(TestFreqList2Gr[["news"]]$OrDF)/10^3 ))
TwoGrunique <- as.numeric(TwoGrunique)

TwoGrhighshare <- c(sprintf("%1.0f", nrow(TestFreqList2Gr[["blogs"]]$LowDF)/10^3 ),sprintf("%1.0f", nrow(TestFreqList2Gr[["twitter"]]$LowDF)/10^3 ),sprintf("%1.0f", nrow(TestFreqList2Gr[["news"]]$LowDF)/10^3 ))
TwoGrhighshare <- as.numeric(TwoGrhighshare)


SummarTable2Gr <- cbind(TwoGrcount,TwoGrunique)
SummarTable2Gr <- cbind(SummarTable2Gr,TwoGrhighshare)
SummarTable2Gr <- as.data.frame(SummarTable2Gr)
SummarTable2Gr$highsharrel <-  with(SummarTable2Gr,sprintf("%0.2f",TwoGrhighshare/(TwoGrcount *10^3) *100))
row.names(SummarTable2Gr) <- c("blogs","twitter","news")
SummarTable2Gr <- SummarTable2Gr[, names(SummarTable2Gr) != "TwoGrhighshare"]
names(SummarTable2Gr) <- c( "2-gram count (10^6)" , "Unique 2-gram (10^3)", "Perc90")


knitr::kable(SummarTable2Gr, caption = "Key summary statistics for 2-grams")
#ngramunique <- ngramunique[order(ngramunique$freq, decreasing = TRUE), ]
#ngramuniqueFreq <-  CutoffTable(ngramunique, "freq", 0.9)
# min_fretable_ngr$share <- min_fretable_ngr[, 2]/nrow(ngramunique) * 100
# names(min_fretable_ngr) <- c("Minimum # of occurences", "# of unique 2-grams", "Share in total # of unique 2-grams")

```


The table above gives some key information on "blogs", "Twitter"" and "news" databases:

- the first column gives, for the databases restricted to alphanumeric characters, the total number of 2-grams
- the second column gives, for the databases restricted to alphanumeric characters, the number of *unique* 2-grams
- the third column gives (in %), the number of *unique* 2-grams corresponding to 90% of *all* 2-gram occurrences. 

We see thus that the vast majority of 2-gram occurrences originates from a small set of *unique* 2-grams. However, as we should have expected, the share of these *unique* 2-grams in the total dataset is much larger (a factor twenty) than for individual words.   

The general skewness of the frequency of 2-grams is confirmed when we look at the summary statistics of the number of counts (see Appendix). These statistics confirm that the vast majority of unique 2-grams appears just a few times in the total database, but that there are some 2-grams that occur with a very high frequency. This is also illustrated by the plot in the Appendix, which shows how the share of 2-grams in the set of *unique* 2-grams changes when we require that they appear at least, respectively, 1, 10, 20, 30, 40 or 50 times in the respective documents. We see that the decrease is even faster than in the case of individuals words.



```{r, echo=FALSE, cache=2}
CounttableTwit2Gr <- CreateCountTable(freq_tables2Gr, "twitter")
```

```{r, echo=FALSE, cache=2}
Counttableblogs2Gr  <- CreateCountTable(freq_tables2Gr, "blogs")
```

```{r, echo=FALSE, cache=2}
Counttablenews2Gr <- CreateCountTable(freq_tables2Gr, "news")
``` 

```{r, echo=FALSE, cache=2}
head_table2Gr <- cbind(head(CounttableTwit2Gr),head(Counttableblogs2Gr))
head_table2Gr <- cbind(head_table2Gr,head(Counttablenews2Gr))


tail_table2gr <- cbind(tail(CounttableTwit2Gr),tail(Counttableblogs2Gr))
tail_table2gr <- cbind(tail_table2gr,tail(Counttablenews2Gr))




#most_freq_words <- sapply(corpusnames, function(x)   freq_tables[[x]][ freq_tables[[x]]$freq == max(freq_tables[[x]]$freq) , "word" ])
most_freq_2gr <- sapply(corpusnames, function(x)   freq_tables2Gr[[x]][ as.numeric(freq_tables2Gr[[x]]$freq) %in% tail_table2gr[, paste("# of counts in", x) ] , "n-gram" ])


head_table2Gr <- head_table2Gr[  , c(1,3,5,6)]
names(head_table2Gr) <- c("# of unique 2-grams in twitter", "# of unique 2-grams in blogs", "# of unique 2-grams in news", "Counts in each dataset")


tail_table2gr <- tail_table2gr[  , c(1,2,4,6)]
row.names(tail_table2gr) <- NULL
names(tail_table2gr) <- c("# of unique 2-grams", "counts in twitter", "counts in blogs", "counts in news")

# freq_count_ngr  <- ngramunique$freq
# FreqPow2Gr <- displ$new(freq_count_ngr)
# #FreqPow2Gr$getXmin()
# #USBlogFreqPow2GrPars <- estimate_pars(USBlogFreqPow2Gr, pars = NULL)
# FreqPow2GrMin <- estimate_xmin(FreqPow2Gr, pars = NULL)
# FreqPow2Gr$setXmin(FreqPow2GrMin)
# FrqvsVal2Gr <- cbind(FreqPow2Gr$internal$freq,FreqPow2Gr$internal$values)
# FrqvsVal2Gr <- data.frame(FrqvsVal2Gr)
# names(FrqvsVal2Gr ) <- c("Number of individuals 2-grams", "Number of counts in the whole database")
```

```{r, echo=FALSE, cache=2}
# head(FrqvsVal2Gr )
# tail(FrqvsVal2Gr )
# most_freq_2gr <- ngramunique[ ngramunique$freq == max(ngramunique$freq) , "n-gram" ]
# most_freq_2gr 
```

Ad we did above for individual words, we have also constructed the  "frequency count table" for 2-grams - see Appendix for the "head" and the "tail" of this table.  These tables show that, for instance, `r sprintf("%1.0f",head_table2Gr[1, 1]/1000)` thousand individual 2-grams appear exactly `r head_table2Gr[1, 4]` time in the Twitter database, while `r tail_table2gr[nrow(tail_table2gr), 1]` individual 2-gram  appears exactly `r sprintf("%1.0f", tail_table2gr[nrow(tail_table2gr), 2])` times in the Twitter database.

A table in Appendix gives the most frequent 2-grams for each dataset. As in the case of individual words there are no surprises. It is interesting to note that they all start with a preposition followed by an article. 




```{r, echo=FALSE}
set.seed(123)
feww2grams <- lapply(corpusnames, VeryInfWords, tables = freq_tables2Gr)
feww2gramstable <- do.call("cbind",feww2grams)
row.names(feww2gramstable) <- NULL
```

Finally, we can have a look at a sample of 2-grams that appear very infrequently in each database (see Appendix). A lot of these 2-grams are meaningless in English, but compared to individual words, there are also some "low frequency" 2-grams that do make sense.


In the plot in the Appendix, we have set out the logs of the variables in the "frequency count table". This confirms that, for 2-grams that appear very frequently, the observations deviate substantially from the predictions of a power law, and  that we cannot thus maintain the hypothesis of a power law - a rigorous hypothesis test falls outside the scope of this paper. 



##Data cleaning

As the next step in our project, we have cleaned the original data: 

- First, we have modified the encoding from "UTF-8" to "latin1" - not doing so would lead to the creation of unreadable characters.
- Second, we have removed any numbers present: these are highly context-dependent and unlikely to have a high predictive value (or, conversely, unlikely to be easy to predict).
- Third, we have removed all the words that are banned by Google for their profane or offensive character.
- Fourth, we have removed all white spaces that have resulted from the previous steps. 

However, punctuation and other non-alphanumeric characters are an integral part of the sentence (especially in the case of Twitter), and have therefore been kept as they were. Meaningless non-alphanumeric characters  are unlikely to appear frequently in the same sequences, and will thus be eliminated by restricting the sets to words and n-grams that appear with a minimal frequency. 

Although foreign words have crept inside the database, we have left them as they were. Indeed, some foreign words are deliberately used in English to convey a specific message. If foreign sentences have been included inadvertently in the database, it is unlikely that similar sentences (or sentences "contaminating" English content) have appeared frequently, and they are also likely to disappear once we eliminate low frequency words and n-grams. 

Next, we have used the "cleaned" version of the database (which differs thus from the "raw" version discussed above) to create a list of words and their occurrence. Moreover, we have constructed a list of all n-grams (for n = 4,3,2) and counted how often each individual n-gram occurs. 
Thus, for instance, the sentence: "This is how to count two-grams" would be split in: "this is", "is how", "how to", "to count", "count two-grams", and then we would count how often each 2-gram "occurs in the dataset.

For the purposes of the predictive model, we have used the Markov assumption, this is, we have assumed that the probability that a given word is the next word in a sentence only depends on the *n* preceding words, rather than on the complete sentence. 

For instance, we have used the 4-grams to predict the probability of a word occurring, conditional on the three preceding words in the sentence. In order to do so, we have split each 4-gram in two strings, one representing the first three words and one the final word (which needs to be predicted). Because we know the count for all 4-grams and all 3-grams, it is  straightforward to calculate the conditional probability that a given word will follow a given 3-gram.

As stated above, because of the general skewness of the distribution of the n-grams and in order to save computer resources, we have limited ourselves to the n-grams that appeared at least 3 times in each dataset. It remains to be tested how this choice affects the overall predictive power of our model.  

For any given sentence, the prediction algorithm will then first predict the three most likely "next" words, first using the last three words of the sentence, then (if no results are obtained from the previous step)  the last two words, etc. 

##Next steps

With the work performed until now, two major steps remain to be done. First, test the actual performance of the model with alternative datasets. Second, to develop the "Shiny" app. It is possible that, due to the technical limitations of Shiny, our model will need to be "culled" in order to be implementable on-line.  

#Appendix


##Summary statistics for words and 2-gram counts

```{r, echo=FALSE, warning =FALSE, message=FALSE}
knitr::kable(    sapply(freq_tables, function(x) summary(x[, "freq"])), caption = "Summary statistics for word counts")
rm(or_tables)
#gc()
```

```{r, echo=FALSE}

knitr::kable(    sapply(freq_tables2Gr, function(x) summary(x[, "freq"])), caption = "Summary statistics for 2-grams")

```



##Share of unique words/2-grams as function of minimal occurence 

```{r, echo=FALSE}
aa <- qplot(MinOccur, data= GlobFreqTable2, geom = "bar", fill = time, weight = Share, position = "dodge", binwidth = 5) + xlab("Minimal occurence of word") + ylab("Share in total number of unique words")

ab <- qplot(MinOccur, data= GlobFreqTable2Gr2, geom = "bar", fill = time, weight = Share, position = "dodge", binwidth = 5) + xlab("Minimal occurence of 2-gr") + ylab("Share in total number of unique 2-grams")

library(grid)
grid.newpage()
pushViewport(viewport(layout = grid.layout(1, 2)))
vplayout <- function(x, y)
viewport(layout.pos.row = x, layout.pos.col = y)
print(aa, vp = vplayout(1, 1))
print(ab, vp = vplayout(1, 2))

```

##Frequency count tables for low frequency words and 2-grams

```{r, echo=FALSE}
knitr::kable(head_table, caption = "Low frequency words")
knitr::kable(head_table2Gr, caption = "Low frequency 2-grams")
```


##Frequency count tables for high frequency words and 2-grams

```{r, echo=FALSE}
knitr::kable(tail_table, caption = "High frequency words")
knitr::kable(tail_table2gr, caption = "High frequency 2-grams")
```

##Sample of low frequency words and 2-grams

```{r, echo=FALSE}
knitr::kable(wordsample, caption ="Random  sample of infrequent words")
knitr::kable(feww2gramstable, caption= "Random sample of infrequent 2-grams")
```


##Most frequents words and 2-grams

`

```{r, echo=FALSE, cache=2}
knitr::kable(most_freq_words, caption= "Most frequent words in the three datasets")
```

```{r, echo=FALSE, cache=2}
knitr::kable(most_freq_2gr , caption= "Most frequent 2-grams in the three datasets")
```


##Graphical representation of the frequency count tables 


In the plot below, we have set out the logs of the variables in the "frequency count table". Thus, on the horizontal axis, we represent the log of the number of different words/2-grams occurring a given number of times in each dataset (where the vertical represents the log of the count of this word/2-grams in the dataset). 

If the counts would follow a power law, the plots would follow a linear trend. However, we observe that this linear trend only holds (approximately) for unique words and 2-grams that occur less than 100 times in the total dataset. After this threshold, we see that there is an increasing spread around the linear trend: for words that occur very often, it is very unlikely that there is another word with exactly the same count in the dataset (and log(Number of words) is thus very likely to be zero).  


```{r, echo=FALSE, cache=2}
a <- qplot(log10(CounttableTwit[,1]),log10(CounttableTwit[,2])   , data= CounttableTwit) +xlab("log(Number of words)") + ylab("log(Counts in Twitter)") 
b <- qplot(log10(Counttableblogs[,1]),log10(Counttableblogs[,2])   , data= Counttableblogs) +xlab("log(Number of words)") + ylab("log(Counts in blogs)") 
c <- qplot(log10(Counttablenews[,1]),log10(Counttablenews[,2])   , data= Counttablenews) +xlab("log(Number of words)") + ylab("log(Counts in news)") 
library(grid)
grid.newpage()
pushViewport(viewport(layout = grid.layout(2, 2)))
vplayout <- function(x, y)
viewport(layout.pos.row = x, layout.pos.col = y)
print(a, vp = vplayout(1, 1))
print(b, vp = vplayout(1, 2))
print(c, vp = vplayout(2, 1))

```




```{r, echo=FALSE, cache=2}
d <- qplot(log10(CounttableTwit2Gr[,1]),log10(CounttableTwit2Gr[,2])   , data= CounttableTwit2Gr) +xlab("log(Number of 2-gr)") + ylab("log(Counts in Twitter)")   
e <- qplot(log10(Counttableblogs2Gr[,1]),log10(Counttableblogs2Gr[,2])   , data= Counttableblogs2Gr) +xlab("log(Number of 2-gr)") + ylab("log(Counts in blogs)") 
f <- qplot(log10(Counttablenews2Gr[,1]),log10(Counttablenews2Gr[,2])   , data= Counttablenews2Gr) +xlab("log(Number of 2-gr)") + ylab("log(Counts in news)") 
library(grid)
grid.newpage()
pushViewport(viewport(layout = grid.layout(2, 2)))
vplayout <- function(x, y)
viewport(layout.pos.row = x, layout.pos.col = y)
print(d, vp = vplayout(1, 1))
print(e, vp = vplayout(1, 2))
print(f, vp = vplayout(2, 1))

```


