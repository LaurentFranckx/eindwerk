---
title: "Milestone report"
author: "Me"
date: "Thursday, March 19, 2015"
output: html_document
---

#Introduction

This paper presents intermediate results in the construction of a predictive text model. The final objective
of this project is to construct a Shiny app which provides users with suggestions for autocompleting a sentence they have started. 

The training data set has been obtained from the Coursera website, but are originally  from a corpus called HC Corpora (www.corpora.heliohost.org). The complete set consists of databases in English, German, Russian and Finnish. In this project we will limit ourselves to the English database. 

The English database itself is composed of three files, each composed of several hundreds of thousands of text lines from blogs, news cites and Twitter. 

As a first step, we will summarize the main descriptive statistics for each dataset, and explore how these compare to each other - these statistics will be provided for individual words and for 2-grams (for a definition, see further). 
As a next step, we will  explain the approach used to the cleaning  and the organisation of the data.
Finally, we will describe the next steps we intend to undertake in order to construct our predictive model. 




#Descriptive statistics for the unique words

Before undertaking any concrete modelling steps, it is important to understand the main features of the data. As we shall see below, our understanding of the data will allow us to drastically save on computer resources without jeopardizing predictive accuracy.

On prior grounds, one would expect the three databases to differ on some important points. For instance, Twitter can be expected to contain relatively more abbreviations, relatively few stopwords, and more non-alphanumerical symbols than the other databases. We would also expect Twitter to contain less grammatically correct sentences. The differences between the "blogs" and the "news" database are probably less pronounced, but one would expect the "news" database to use a more formal vocabulary, and to contain less spelling and grammatical mistakes. Therefore, we will analyse the key statistics separately.  


```{r, echo=FALSE, cache=2, message=FALSE, warning=FALSE}
library("poweRlaw")
library("plyr")
library("ggplot2")

CutoffTable <- function(df, freq_val, value){
  df$sum_of_freq <- cumsum(df[, freq_val])
  total_count <- sum(df[, freq_val])
  df$sum_of_shares <- df$sum_of_freq/total_count
  df_low <- df[df$sum_of_shares < value,   ]
  returnlist <- list(df, total_count, df_low)
  names(returnlist) <- c("OrDF", "Count","LowDF")
  return(returnlist)
}
setwd("D:/coursera/dsc_capstone")
folder_txt <- file.path("Coursera-SwiftKey","final","en_US")

corpusnames <- c("news","twitter","blogs")

or_tables <- list()
freq_tables <- list()
min_fretables <- list()

for(name in corpusnames){
  or_tables[[name]] <- readLines(file.path(folder_txt, paste("en_US", name, "txt", sep =".")))
  TestFreqTable <- read.table(paste("US.", name , "unique.txt", sep=""), stringsAsFactors = FALSE)
  names(TestFreqTable) <- c("freq", "word")
  min_fretable <- rbind(c(2, nrow(TestFreqTable[TestFreqTable$freq > 1, ])), c(10,nrow( TestFreqTable[TestFreqTable$freq > 9, ])))
  #min_fretable <- rbind(min_fretable, c(10, nrow(TestFreqTable[TestFreqTable$freq > 9, ])))
  min_fretable <- rbind(min_fretable, c(20, nrow(TestFreqTable[TestFreqTable$freq > 19, ])))
  min_fretable <- rbind(min_fretable, c(30, nrow(TestFreqTable[TestFreqTable$freq > 29,])))
  min_fretable <- rbind(min_fretable, c(40, nrow(TestFreqTable[TestFreqTable$freq > 39, ])))
  min_fretable <- rbind(min_fretable, c(50, nrow(TestFreqTable[TestFreqTable$freq > 49, ])))
#  min_fretable <- rbind(min_fretable, c(40, nrow(TestFreqTable[TestFreqTable$freq > 39, ])))
  min_fretable <- as.data.frame(min_fretable)
  min_fretable$share <- min_fretable[, 2]/nrow(TestFreqTable) * 100
  names(min_fretable) <- c("MinOccur", paste("Uniq", name,sep=""), paste("Share",name,sep=""))
  min_fretable <- as.data.frame(min_fretable)
  min_fretables[[name]] <- min_fretable
  freq_tables[[name]]  <- TestFreqTable
}

GlobFreqTable <- Reduce("join", min_fretables)
GlobFreqTable2 <- reshape(GlobFreqTable, idvar ="MinOccur", times = corpusnames , varying = paste("Share",corpusnames,sep="") , v.names = "Share", direction = "long")



#names(TestFreqTable) <- c("freq", "word")
#GlobFreqTable <- GlobFreqTable[order(GlobFreqTable$freq, decreasing = TRUE), ]
TestFreqList <- lapply(freq_tables, CutoffTable, "freq", 0.9)

textlines <- c(sprintf("%1.1f", length(or_tables[["blogs"]])/10^6 ),sprintf("%1.1f", length(or_tables[["twitter"]])/10^6 ),sprintf("%1.1f", length(or_tables[["news"]])/10^6 ))
textlines <- as.numeric(textlines)

wordcount <- c(sprintf("%1.1f", TestFreqList[["blogs"]]$Count/10^6 ),sprintf("%1.1f", TestFreqList[["twitter"]]$Count/10^6 ),sprintf("%1.1f", TestFreqList[["news"]]$Count/10^6 ))
wordcount <- as.numeric(wordcount)

uniquewords <- c(sprintf("%1.1f", nrow(TestFreqList[["blogs"]]$OrDF)/10^3 ),sprintf("%1.1f", nrow(TestFreqList[["twitter"]]$OrDF)/10^3 ),sprintf("%1.1f", nrow(TestFreqList[["news"]]$OrDF)/10^3 ))
uniquewords <- as.numeric(uniquewords)

highshare <- c(sprintf("%1.0f", nrow(TestFreqList[["blogs"]]$LowDF)/10^3 ),sprintf("%1.0f", nrow(TestFreqList[["twitter"]]$LowDF)/10^3 ),sprintf("%1.0f", nrow(TestFreqList[["news"]]$LowDF)/10^3 ))
highshare <- as.numeric(highshare)

SummarTableWords <- cbind(textlines,wordcount)
SummarTableWords <- cbind(SummarTableWords,uniquewords)
SummarTableWords <- cbind(SummarTableWords,highshare)
SummarTableWords <- as.data.frame(SummarTableWords)
SummarTableWords$highsharrel <-  with(SummarTableWords,sprintf("%0.2f",highshare/(wordcount *10^3) *100))
row.names(SummarTableWords) <- c("blogs","twitter","news")
SummarTableWords <- SummarTableWords[, names(SummarTableWords) != "highshare"]
names(SummarTableWords) <- c("Lines of text (10^6)", "Word count (10^6)" , "Unique words (10^3)", "Perc90")


knitr::kable(SummarTableWords, caption = "Key summary statistics for unique words")

#GlobFreqTable$logMinOccur <- log10(GlobFreqTable$MinOccur)

```

The table above gives some key information on "blogs", "Twitter"" and "news" databases

- the first column gives, for the "uncleaned" version of the databases, the number of lines of text
- the second column gives, for the databases restricted to alphanumeric characters, the total number of words
- the third column gives, for the databases restricted to alphanumeric characters, the number of *unique* words
- the fourth column gives (in %), the number of *unique* words corresponding to 90% of *all* word occurences. 

We see thus that the vast majority of word occurences originates from a small dictionary. 

The general skewness of the frequency of words is confirmed when we look at the summary statistics of the number of counts:  

```{r, echo=FALSE, warning =FALSE, message=FALSE}
knitr::kable(    sapply(freq_tables, function(x) summary(x[, "freq"])), caption = "Summary statistics for word counts")
rm(or_tables)
gc()
```

These statistics confirm that the vast majority of unique words appears just a few times in the total database, but that there are some words that occur with a very high frequency. This is also illustrated by the plot below, which shows how many words appear at least, respectively, 2, 10, 20, 30, 40 or 50 times in the respective documents.


```{r, echo=FALSE}
qplot(MinOccur, data= GlobFreqTable2, geom = "bar", fill = time, weight = Share, position = "dodge", binwidth = 5) + xlab("Minimal occurence of word") + ylab("Count in corpus")
```


Still another way to look at the data is to calculate how many individual words appear in the database with a given count, thus for instance: how many *unique* words appear exactly 1, 2, 3 etc times? We represent here the first five and the last five lines of the table containing this information (henceforth the "frequency count table"). 



```{r, echo=FALSE, cache=2}


CreateCountTable <- function(tablelist, corpus){
  freqtable <- tablelist[[corpus]]
  freq_count <- freqtable$freq
  FreqPow <- displ$new(freq_count)
#FreqPow$getXmin()
#USBlogFreqPowPars <- estimate_pars(USBlogFreqPow, pars = NULL)

  #test: how does this impact on model performance
#   FreqPowMin <- estimate_xmin(FreqPow, pars = NULL)
#   FreqPow$setXmin(FreqPowMin)


  FrqvsVal <- cbind(FreqPow$internal$freq,FreqPow$internal$values)
  FrqvsVal <- as.data.frame(FrqvsVal)
  names(FrqvsVal ) <- c(paste("# of words/n-grams", corpus), paste("# of counts in", corpus))
  return(FrqvsVal) 
}

CounttableTwit <- CreateCountTable(freq_tables, "twitter")
Counttableblogs <- CreateCountTable(freq_tables, "blogs")
Counttablenews <- CreateCountTable(freq_tables, "news")


```

```{r, echo=FALSE, cache=2}
head_table <- cbind(head(CounttableTwit),head(Counttableblogs))
head_table <- cbind(head_table,head(Counttablenews))
knitr::kable(head_table, caption = "Low frequency words")

tail_table <- cbind(tail(CounttableTwit),tail(Counttableblogs))
tail_table <- cbind(tail_table,tail(Counttablenews))
knitr::kable(tail_table, caption = "High frequency words")



#most_freq_words <- sapply(corpusnames, function(x)   freq_tables[[x]][ freq_tables[[x]]$freq == max(freq_tables[[x]]$freq) , "word" ])
most_freq_words <- sapply(corpusnames, function(x)   freq_tables[[x]][ freq_tables[[x]]$freq %in% tail_table[, paste("# of counts in", x) ] , "word" ])


```

Thus, for instance, `r sprintf("%1.0f",head_table[1, 1]/1000)` thousand individual words appear exactly `r head_table[1, 2]` time in the Twitter database, while `r tail_table[nrow(tail_table), 1]` word  appears exactly `r tail_table[nrow(tail_table), 2]` times in the Twitter database.

For each of the three data files, the 6 most frequent words are:

```{r, echo=FALSE, cache=2}
knitr::kable(most_freq_words, caption= "Most frequent words in the three databases")
```

Finally, we can have a look at a random sample of words that appear very infrequently in the database (here, less than 5 times): 

```{r, echo=FALSE, cache=2}
set.seed(123)

VeryInfWords <- function( corpus, tables, lessthan = 5, samplesize = 10){
  freqtable <- tables[[corpus]]
  low_frq <- freqtable[ freqtable$freq < lessthan , ]
  low_frq <- low_frq[sample(1:nrow(low_frq), samplesize)  , ]
  names(low_frq) <- c(paste("freq in", corpus), paste("word from", corpus))
  return(low_frq)
}

fewwords <- lapply(corpusnames, VeryInfWords, tables = freq_tables)
wordsample <- do.call("cbind",fewwords)
knitr::kable(wordsample, caption ="Random  sample of low frequency words")

# head(TestFreqTable[ TestFreqTable$freq == min(TestFreqTable$freq) , ], 3)
# 
# head(TestFreqTable[ TestFreqTable$freq == 2 , ], 3)
# 
# head(TestFreqTable[ TestFreqTable$freq == 3 , ], 3)
# 
# head(TestFreqTable[ TestFreqTable$freq == 4 , ], 3)
```

These words are generally meaningless in English, or are obvious spelling mistakes.  

It has been suggested in the literature that the counts of individuals words in a corpus generally follows a power law, i.e. that the frequency of a word is inversely proportional to its frequency rank. Thus, the second most frequent word occurs half as often the most frequent word, the third most frequent word occurs one third as often as the most frequent word, etc - see http://en.wikipedia.org/wiki/Power_law. 


However, a formal hypothesis has rejected this possibility for the databases we use here (details are available on request). 
In the plot below, we have set out the logs of the variables in the "frequency count table". This confirms that, for words that appear very frequently, the observations deviate substantially from the predictions of a power law. 



```{r, echo=FALSE, cache=2}
qplot(log(CounttableTwit[,1]),log(CounttableTwit[,2])   , data= CounttableTwit)  
qplot(log(Counttableblogs[,1]),log(Counttableblogs[,2])   , data= Counttableblogs)
qplot(log(Counttablenews[,1]),log(Counttablenews[,2])   , data= Counttablenews)

```


Nevertheless, we can conclude that leaving out words that do not occur with a minimal frequency in each dataset, is unlikely to affect the predictive performance of the model. We will discuss below how we have determined this "minimal" frequency.


#Descriptive statistics for 2-grams

The next question we need to consider is the distribution of n-grams, i.e.  contiguous sequences of n items in the database. As we shall discuss below, they will be the cornerstone of the predictive model. 

For the purposes of this project, we have constructed 2-grams, 3-grams and 4-grams. 

As a full discussion of the properties of all n-grams would take us beyond the scope of this paper, we focus here on the analysis of 2-grams. A full analysis is available on request. 

As in the analysis of individual words, we have performed a separate analysis for the "blogs", "Twitter"" and "news" datasets. 



```{r, echo=FALSE, cache=2, message=FALSE}
#ngramunique <- read.table("US.twitterTokened2Gr_uniq.txt", stringsAsFactors = FALSE)
#ngramunique <- readLines("US.twitterTokened4Gr_uniq.txt")
or_tables2Gr <- list()
freq_tables2Gr <- list()
min_fretables2Gr <- list()

for(name in corpusnames){
#  or_tables2Gr[[name]] <- readLines(file.path(folder_txt, paste("en_US", name, "txt", sep =".")))
  ngramunique <- read.table(paste("US.", name , "Tokened2Gr_uniq.txt", sep=""), stringsAsFactors = FALSE)
  names(ngramunique) <- c("freq", "n-gram")
  ngramunique <- ngramunique[complete.cases(ngramunique),]
  min_fretable <- rbind(c(2, nrow(ngramunique[ngramunique$freq > 1, ])), c(10,nrow( ngramunique[ngramunique$freq > 9, ])))
  #min_fretable <- rbind(min_fretable, c(10, nrow(ngramunique[ngramunique$freq > 9, ])))
  min_fretable <- rbind(min_fretable, c(20, nrow(ngramunique[ngramunique$freq > 19, ])))
  min_fretable <- rbind(min_fretable, c(30, nrow(ngramunique[ngramunique$freq > 29,])))
  min_fretable <- rbind(min_fretable, c(40, nrow(ngramunique[ngramunique$freq > 39, ])))
  min_fretable <- rbind(min_fretable, c(50, nrow(ngramunique[ngramunique$freq > 49, ])))
#  min_fretable <- rbind(min_fretable, c(40, nrow(ngramunique[ngramunique$freq > 39, ])))
  min_fretable <- as.data.frame(min_fretable)
  min_fretable$share <- min_fretable[, 2]/nrow(ngramunique) * 100
  names(min_fretable) <- c("MinOccur", paste("Uniq", name,sep=""), paste("Share",name,sep=""))
  min_fretable <- as.data.frame(min_fretable)
  min_fretables2Gr[[name]] <- min_fretable
  freq_tables2Gr[[name]]  <- ngramunique
}

GlobFreqTable2Gr <- Reduce("join", min_fretables2Gr)
GlobFreqTable2Gr2  <- reshape(GlobFreqTable2Gr, idvar ="MinOccur", times = corpusnames , varying = paste("Share",corpusnames,sep="") , v.names = "Share", direction = "long")


TestFreqList2Gr <- lapply(freq_tables2Gr, CutoffTable, "freq", 0.9)

TwoGrcount <- c(sprintf("%1.1f", TestFreqList2Gr[["blogs"]]$Count/10^6 ),sprintf("%1.1f", TestFreqList2Gr[["twitter"]]$Count/10^6 ),sprintf("%1.1f", TestFreqList2Gr[["news"]]$Count/10^6 ))
TwoGrcount <- as.numeric(TwoGrcount)

TwoGrunique <- c(sprintf("%1.1f", nrow(TestFreqList2Gr[["blogs"]]$OrDF)/10^3 ),sprintf("%1.1f", nrow(TestFreqList2Gr[["twitter"]]$OrDF)/10^3 ),sprintf("%1.1f", nrow(TestFreqList2Gr[["news"]]$OrDF)/10^3 ))
TwoGrunique <- as.numeric(TwoGrunique)

TwoGrhighshare <- c(sprintf("%1.0f", nrow(TestFreqList2Gr[["blogs"]]$LowDF)/10^3 ),sprintf("%1.0f", nrow(TestFreqList2Gr[["twitter"]]$LowDF)/10^3 ),sprintf("%1.0f", nrow(TestFreqList2Gr[["news"]]$LowDF)/10^3 ))
TwoGrhighshare <- as.numeric(TwoGrhighshare)


SummarTable2Gr <- cbind(TwoGrcount,TwoGrunique)
SummarTable2Gr <- cbind(SummarTable2Gr,TwoGrhighshare)
SummarTable2Gr <- as.data.frame(SummarTable2Gr)
SummarTable2Gr$highsharrel <-  with(SummarTable2Gr,sprintf("%0.2f",TwoGrhighshare/(TwoGrcount *10^3) *100))
row.names(SummarTable2Gr) <- c("blogs","twitter","news")
SummarTable2Gr <- SummarTable2Gr[, names(SummarTable2Gr) != "TwoGrhighshare"]
names(SummarTable2Gr) <- c( "2-gram count (10^6)" , "Unique 2-gram (10^3)", "Perc90")


knitr::kable(SummarTable2Gr, caption = "Key summary statistics for 2-grams")
#ngramunique <- ngramunique[order(ngramunique$freq, decreasing = TRUE), ]
#ngramuniqueFreq <-  CutoffTable(ngramunique, "freq", 0.9)
# min_fretable_ngr$share <- min_fretable_ngr[, 2]/nrow(ngramunique) * 100
# names(min_fretable_ngr) <- c("Minimum # of occurences", "# of unique 2-grams", "Share in total # of unique 2-grams")

```


The table above gives some key information on "blogs", "Twitter"" and "news" databases:

- the first column gives, for the databases restricted to alphanumeric characters, the total number of 2-grams
- the second column gives, for the databases restricted to alphanumeric characters, the number of *unique* 2-grams
- the third column gives (in %), the number of *unique* 2-grams corresponding to 90% of *all* 2-gram occurences. 

We see thus that the vast majority of 2-gram occurences originates from a small set of *unique* 2-grams. However, as we should have expected, the share of these *unique* 2-grams in the total dataset is much larger than for individual words.   

The general skewness of the frequency of 2-grams is confirmed when we look at the summary statistics of the number of counts:  

```{r, echo=FALSE}

knitr::kable(    sapply(freq_tables2Gr, function(x) summary(x[, "freq"])), caption = "Summary statistics for 2-grams")

```


These statistics confirm that the vast majority of unique 2-grams appears just a few times in the total database, but that there are some 2-grams that occur with a very high frequency. This is also illustrated by the plot below, which shows how many 2-grams appear at least, respectively, 2, 10, 20, 30, 40 or 50 times in the respective documents.


```{r, echo=FALSE}
qplot(MinOccur, data= GlobFreqTable2Gr2, geom = "bar", fill = time, weight = Share, position = "dodge", binwidth = 5) + xlab("Minimal occurence of 2-gram") + ylab("Count in corpus")
```

Ad we did above for individual words, we also represent the  "frequency count table" for 2-grams: 



```{r, echo=FALSE, cache=2}
CounttableTwit2Gr <- CreateCountTable(freq_tables2Gr, "twitter")
```

```{r, echo=FALSE, cache=2}
Counttableblogs2Gr  <- CreateCountTable(freq_tables2Gr, "blogs")
```

```{r, echo=FALSE, cache=2}
Counttablenews2Gr <- CreateCountTable(freq_tables2Gr, "news")
``` 

```{r, echo=FALSE, cache=2}
head_table2Gr <- cbind(head(CounttableTwit2Gr),head(Counttableblogs2Gr))
head_table2Gr <- cbind(head_table2Gr,head(Counttablenews2Gr))
knitr::kable(head_table2Gr, caption = "Low frequency 2-grams")

tail_table2gr <- cbind(tail(CounttableTwit2Gr),tail(Counttableblogs2Gr))
tail_table2gr <- cbind(tail_table2gr,tail(Counttablenews2Gr))
knitr::kable(tail_table2gr, caption = "High frequency 2-grams")



#most_freq_words <- sapply(corpusnames, function(x)   freq_tables[[x]][ freq_tables[[x]]$freq == max(freq_tables[[x]]$freq) , "word" ])
most_freq_2gr <- sapply(corpusnames, function(x)   freq_tables2Gr[[x]][ as.numeric(freq_tables2Gr[[x]]$freq) %in% tail_table2gr[, paste("# of counts in", x) ] , "n-gram" ])



# freq_count_ngr  <- ngramunique$freq
# FreqPow2Gr <- displ$new(freq_count_ngr)
# #FreqPow2Gr$getXmin()
# #USBlogFreqPow2GrPars <- estimate_pars(USBlogFreqPow2Gr, pars = NULL)
# FreqPow2GrMin <- estimate_xmin(FreqPow2Gr, pars = NULL)
# FreqPow2Gr$setXmin(FreqPow2GrMin)
# FrqvsVal2Gr <- cbind(FreqPow2Gr$internal$freq,FreqPow2Gr$internal$values)
# FrqvsVal2Gr <- data.frame(FrqvsVal2Gr)
# names(FrqvsVal2Gr ) <- c("Number of individuals 2-grams", "Number of counts in the whole database")
```

```{r, echo=FALSE, cache=2}
# head(FrqvsVal2Gr )
# tail(FrqvsVal2Gr )
# most_freq_2gr <- ngramunique[ ngramunique$freq == max(ngramunique$freq) , "n-gram" ]
# most_freq_2gr 
```

Thus, for instance, `r sprintf("%1.0f",head_table2Gr[1, 1]/1000)` thousand individual 2-grams appear exactly `r head_table[1, 2]` time in the Twitter database, while `r tail_table2gr[nrow(tail_table2gr), 1]` individual 2-grams  appears exactly `r tail_table2gr[nrow(tail_table2gr), 2]` times in the Twitter database.

For the three databases the 6 most frequent 2-grams are:

```{r, echo=FALSE, cache=2}
knitr::kable(most_freq_2gr , caption= "Most frequent 2-gram in the three databases")
```



Finally, we can have a look at a sample of 2-grams that appear very infrequently in each database: 

```{r, echo=FALSE, cache=2}
set.seed(123)
feww2grams <- lapply(corpusnames, VeryInfWords, tables = freq_tables2Gr)
feww2gramstable <- do.call("cbind",feww2grams)
knitr::kable(feww2gramstable, caption= "Sample of infrequent 2-grams in the three databases")
```

We see that there are still a lot of these 2-grams that make little sense in English, but compared to individual words, there are also some "low frequency" 2-grams that do make sense.


In the plot below, we have set out the logs of the variables in the "frequency count table". This confirms that, for 2-grams that appear very frequently, the observations deviate substantially from the predictions of a power law. 



```{r, echo=FALSE, cache=2}
qplot(log(CounttableTwit2Gr[,1]),log(CounttableTwit2Gr[,2])   , data= CounttableTwit2Gr)  
qplot(log(Counttableblogs2Gr[,1]),log(Counttableblogs2Gr[,2])   , data= Counttableblogs2Gr)
qplot(log(Counttablenews2Gr[,1]),log(Counttablenews2Gr[,2])   , data= Counttablenews2Gr)
rm()
```

#Data cleaning

As the next step in our project, we have cleaned the original data: 

- First, we have modified the encoding from "UTF-8" to "latin1" - not doing so would lead to the creation of unreadable characters.
- Second, we have removed any numbers present: these are highly context-dependent and unlikely to have a high predictive value (or, conversely, easy to predict)
- Third we have removed all the words that are banned by Google for their profane or offensive character.
- Fourth, we have removed all white spaces that have resulted from the previous steps. 

Howevern, punctuation and other non-alphanumeric characters are an integral part of the sentence (especially in the case of Twitter), and have therefore been kept as they were. Meaningless non-alphanumeric characters  are unlikely to appear frequently in the same sequences, and will thus be eliminated in the next stage.   

Although foreign words have crept inside the database, we have left them as they were. Indeed, some foreign words are deliberately used in English to convey a specific message. If foreign sentences have been included inadvertently in the database, it is unlikely that similar sentences (or sentences "contaminating" English content) have appeared frequently, and they are likely to be eliminated in the next step of the analysis. 

Next, we have used the "cleaned" version of the database to create a list of words and their occurence. Moreover, we have constructed a list of all n-grams (for n = 4,3,2) and counted how often each individual n-gram occurs. 
Thus, for instance, the sentence: "This is how to count two-grams" would be split in: "this is", "is how", "how to", "to count", "count two-grams", and then we would count how often the 2-gram "this is" occurs in the dataset.

For the purposes of the predictive model, we have used the Markov assumption, this is, we have assumed that the probability that a given word is the next word in a sentence only depends on the n preceding words, rather than on the complete sentence. 

For instance, we have used the 4-grams to predict the probability of a word occuring conditional on the three preceeding words in the sentence. In order to do so, we have split each 4-gram in two strings, one representing the first three words and one the final word (which needs to be predicted). Because we know the count for all 4-grams and all 3-grams, it is then straighforward to calculate the conditional probability that a given word will follow a given 3-gram.

As stated above, because of the general skewness of the distribution of the n-grams and in order to save computer resources, we have limited ourselves to the n-grams that appeared at least 3 times in each dataset. 

For any given sentence, the prediction algorithm will then first predict the three most likely "next" words, first using the last three words of the sentence, then (if no results are obtained from the previous step) from the last two words, etc. 

#Next steps

With the work performed until now, two major steps remain to be done. First, test the actual performance of the model with alternative datasets. Second, to develop the "Shiny" app. It is possible that, due to the technical limitations of Shiny, our model will need to be "culled" in order to be implementable on-line.  
